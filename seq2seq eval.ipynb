{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Educativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "4768it [00:03, 1310.11it/s]\n",
      "607it [00:00, 1394.52it/s]\n",
      "607it [00:00, 1353.53it/s]\n",
      "100%|████████████████████████████████████| 5375/5375 [00:00<00:00, 81738.23it/s]\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"training.py\", line 150, in <module>\n",
      "    fire.Fire(main)\n",
      "  File \"/home/krivas/anaconda3/lib/python3.7/site-packages/fire/core.py\", line 127, in Fire\n",
      "    component_trace = _Fire(component, args, context, name)\n",
      "  File \"/home/krivas/anaconda3/lib/python3.7/site-packages/fire/core.py\", line 366, in _Fire\n",
      "    component, remaining_args)\n",
      "  File \"/home/krivas/anaconda3/lib/python3.7/site-packages/fire/core.py\", line 542, in _CallCallable\n",
      "    result = fn(*varargs, **kwargs)\n",
      "  File \"training.py\", line 141, in main\n",
      "    hypothesis = (' '.join(predictor.predict_instance(instance)['predicted_tokens'])).replace('@@UNKNOWN@@', '').replace('@@ ', '').split()\n",
      "  File \"/home/krivas/anaconda3/lib/python3.7/site-packages/allennlp/predictors/predictor.py\", line 95, in predict_instance\n",
      "    outputs = self._model.forward_on_instance(instance)\n",
      "  File \"/home/krivas/anaconda3/lib/python3.7/site-packages/allennlp/models/model.py\", line 124, in forward_on_instance\n",
      "    return self.forward_on_instances([instance])[0]\n",
      "  File \"/home/krivas/anaconda3/lib/python3.7/site-packages/allennlp/models/model.py\", line 153, in forward_on_instances\n",
      "    outputs = self.decode(self(**model_input))\n",
      "  File \"/home/krivas/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 489, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/krivas/anaconda3/lib/python3.7/site-packages/allennlp/models/encoder_decoders/simple_seq2seq.py\", line 231, in forward\n",
      "    self._bleu(best_predictions, target_tokens[\"tokens\"])\n",
      "  File \"/home/krivas/anaconda3/lib/python3.7/site-packages/allennlp/training/metrics/bleu.py\", line 132, in __call__\n",
      "    predictions, gold_targets = self.unwrap_to_tensors(predictions, gold_targets)\n",
      "  File \"/home/krivas/anaconda3/lib/python3.7/site-packages/allennlp/training/metrics/metric.py\", line 49, in <genexpr>\n",
      "    return (x.detach().cpu() if isinstance(x, torch.Tensor) else x for x in tensors)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python training.py --name_file=Educativo/educativo  --dir_files=ParallelCorpora --dir_results=results_educativo_5/bpe_5000 --evaluate=True --bpe=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import itertools\n",
    "from typing import Iterator, List, Dict\n",
    "import fire\n",
    "from allennlp.common.util import prepare_environment\n",
    "from allennlp.data.dataset_readers.seq2seq import Seq2SeqDatasetReader\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.data.fields import TextField, IndexField\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data import Instance\n",
    "\n",
    "from allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\n",
    "from allennlp.data.tokenizers.word_tokenizer import WordTokenizer\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.nn.activations import Activation\n",
    "from allennlp.models.encoder_decoders.simple_seq2seq import SimpleSeq2Seq\n",
    "from allennlp.models import DecomposableAttention\n",
    "from allennlp.modules.attention import LinearAttention, BilinearAttention, DotProductAttention\n",
    "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper, StackedSelfAttentionEncoder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.predictors import SimpleSeq2SeqPredictor\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.training.metrics.bleu import BLEU\n",
    "from allennlp.common.params import Params\n",
    "\n",
    "prepare_environment(Params({}))\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "EN_EMBEDDING_DIM = 128\n",
    "ZH_EMBEDDING_DIM = 128\n",
    "CUDA_DEVICE = 0\n",
    "\n",
    "def main(name_file='Educativo/educativo', dir_files='ParallelCorpora', dir_results='results_educativo_4/bpe_5000', cuda_id=0, hidden_dim=1024, evaluate=True, bpe=False):\n",
    "    \n",
    "    dir_train = os.path.join(dir_files, name_file)\n",
    "    dir_test = os.path.join(dir_files, name_file)\n",
    "    os.makedirs(dir_results, exist_ok=True)\n",
    "    \n",
    "    reader = Seq2SeqDatasetReader(\n",
    "        source_tokenizer=WordTokenizer(),\n",
    "        target_tokenizer=WordTokenizer(),\n",
    "        delimiter='\\t',\n",
    "        source_token_indexers={'tokens': SingleIdTokenIndexer()},\n",
    "        target_token_indexers={'tokens': SingleIdTokenIndexer(namespace='target_tokens')})\n",
    "    \n",
    "    if bpe:\n",
    "        train_dataset = reader.read(os.path.join(dir_files, name_file + '_train.bpe.tsv'))\n",
    "        validation_dataset = reader.read(os.path.join(dir_files, name_file + '_val.bpe.tsv'))\n",
    "        test_dataset = reader.read(os.path.join(dir_files, name_file + '_test.bpe.tsv'))\n",
    "    else:\n",
    "        train_dataset = reader.read(os.path.join(dir_files, name_file + '_train.tsv'))\n",
    "        validation_dataset = reader.read(os.path.join(dir_files, name_file + '_val.tsv'))\n",
    "        test_dataset = reader.read(os.path.join(dir_files, name_file + '_test.tsv'))\n",
    "    \n",
    "    vocab = Vocabulary.from_instances(train_dataset + validation_dataset,\n",
    "                                      min_count={'tokens': 3, 'target_tokens': 3})\n",
    "\n",
    "    en_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                             embedding_dim=EN_EMBEDDING_DIM)\n",
    "    print('size of vocab:', vocab)\n",
    "\n",
    "    encoder = PytorchSeq2SeqWrapper(torch.nn.GRU(EN_EMBEDDING_DIM, hidden_dim, dropout=0.25, num_layers=2, bidirectional=True, batch_first=True))\n",
    "\n",
    "    source_embedder = BasicTextFieldEmbedder({\"tokens\": en_embedding})\n",
    "    # attention = LinearAttention(HIDDEN_DIM, HIDDEN_DIM, activation=Activation.by_name('tanh')())\n",
    "    # attention = BilinearAttention(HIDDEN_DIM, HIDDEN_DIM)\n",
    "    attention = DotProductAttention()\n",
    "\n",
    "    max_decoding_steps = 100   # TODO: make this variable\n",
    "    model = SimpleSeq2Seq(vocab, source_embedder, encoder, max_decoding_steps,\n",
    "                          target_embedding_dim=ZH_EMBEDDING_DIM,\n",
    "                          target_namespace='target_tokens',\n",
    "                          attention=attention,\n",
    "                          beam_size=1,\n",
    "                          use_bleu=True).cuda()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    iterator = BucketIterator(batch_size=100, sorting_keys=[(\"source_tokens\", \"num_tokens\")])\n",
    "\n",
    "    iterator.index_with(vocab)\n",
    "\n",
    "    if not evaluate:\n",
    "        trainer = Trainer(model=model,\n",
    "                          optimizer=optimizer,\n",
    "                          iterator=iterator,\n",
    "                          train_dataset=train_dataset,\n",
    "                          #validation_metric='+BLEU',\n",
    "                          validation_dataset=validation_dataset,\n",
    "                          num_epochs=40,\n",
    "                          serialization_dir=dir_results,\n",
    "                          cuda_device=cuda_id)\n",
    "\n",
    "        trainer.train()\n",
    "        \n",
    "        \n",
    "        \n",
    "    #reader = CustomSeq2SeqDatasetReader(\n",
    "    #    vocab,\n",
    "    #    source_tokenizer=WordTokenizer(),\n",
    "    #    target_tokenizer=WordTokenizer(),\n",
    "    #    delimiter='\\t',\n",
    "    #    source_token_indexers={'tokens': SingleIdTokenIndexer('source_tokens')},\n",
    "    #    target_token_indexers={'tokens': SingleIdTokenIndexer(namespace='target_tokens')})\n",
    "    \n",
    "    if bpe:\n",
    "        test_dataset = reader.read(os.path.join(dir_files, name_file + '_test.bpe.tsv'))\n",
    "    else:\n",
    "        test_dataset = reader.read(os.path.join(dir_files, name_file + '_test.tsv'))\n",
    "\n",
    "    with open(os.path.join(dir_results, \"best.th\"), 'rb') as f:\n",
    "        model.load_state_dict(torch.load(f))\n",
    "    model.eval()\n",
    "    predictor = SimpleSeq2SeqPredictor(model, reader)\n",
    "    total_bleu = 0\n",
    "    len_test = 0\n",
    "    for instance in test_dataset:\n",
    "\n",
    "        hypothesis = (' '.join(predictor.predict_instance(instance)['predicted_tokens'])).replace('@@UNKNOWN@@', '').replace('@@ ', '').split()\n",
    "        reference = (' '.join([token.text for token in instance['target_tokens'].tokens[1:-1]]).replace('@@ ', '')).split()\n",
    "        print(instance['source_tokens'].tokens[1:-1])\n",
    "        print(hypothesis)\n",
    "        print(reference)\n",
    "        print()\n",
    "        \n",
    "        if len(reference) and len(instance['source_tokens'].tokens[1:-1]):\n",
    "            len_test += 1\n",
    "\n",
    "            total_bleu += nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "    \n",
    "    print('=================================')\n",
    "    print('BLEU test:', total_bleu / len_test)\n",
    "                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4768it [00:03, 1250.56it/s]\n",
      "607it [00:00, 1412.75it/s]\n",
      "607it [00:00, 1376.78it/s]\n",
      "100%|██████████| 5375/5375 [00:00<00:00, 81650.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocab: Vocabulary with namespaces:\n",
      " \tNon Padded Namespaces: {'*tags', '*labels'}\n",
      " \tNamespace: tokens, Size: 1671 \n",
      " \tNamespace: target_tokens, Size: 1417 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "607it [00:00, 906.28it/s] \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SimpleSeq2Seq:\n\tsize mismatch for _source_embedder.token_embedder_tokens.weight: copying a param with shape torch.Size([866, 128]) from checkpoint, the shape in current model is torch.Size([1671, 128]).\n\tsize mismatch for _target_embedder.weight: copying a param with shape torch.Size([770, 128]) from checkpoint, the shape in current model is torch.Size([1417, 128]).\n\tsize mismatch for _output_projection_layer.weight: copying a param with shape torch.Size([770, 2048]) from checkpoint, the shape in current model is torch.Size([1417, 2048]).\n\tsize mismatch for _output_projection_layer.bias: copying a param with shape torch.Size([770]) from checkpoint, the shape in current model is torch.Size([1417]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c46013a5e72c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'results_flashcards_6/bpe_5000'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-f0ff6c30fe11>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(name_file, dir_files, dir_results, cuda_id, hidden_dim, evaluate, bpe)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"best.th\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleSeq2SeqPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 769\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_named_members\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SimpleSeq2Seq:\n\tsize mismatch for _source_embedder.token_embedder_tokens.weight: copying a param with shape torch.Size([866, 128]) from checkpoint, the shape in current model is torch.Size([1671, 128]).\n\tsize mismatch for _target_embedder.weight: copying a param with shape torch.Size([770, 128]) from checkpoint, the shape in current model is torch.Size([1417, 128]).\n\tsize mismatch for _output_projection_layer.weight: copying a param with shape torch.Size([770, 2048]) from checkpoint, the shape in current model is torch.Size([1417, 2048]).\n\tsize mismatch for _output_projection_layer.bias: copying a param with shape torch.Size([770]) from checkpoint, the shape in current model is torch.Size([1417])."
     ]
    }
   ],
   "source": [
    "main(dir_results='results_flashcards_6/bpe_5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "4768it [00:04, 1159.86it/s]\n",
      "607it [00:00, 1251.29it/s]\n",
      "607it [00:00, 1185.72it/s]\n",
      "100%|████████████████████████████████████| 5375/5375 [00:00<00:00, 72537.09it/s]\n",
      "=================================\n",
      "BLEU test: 0.03533705687076492\n"
     ]
    }
   ],
   "source": [
    "!python training.py --name_file=Educativo/all/educativo_5000  --dir_files=ParallelCorpora --dir_results=results_educativo_2/bpe_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "4768it [00:03, 1246.85it/s]\n",
      "607it [00:00, 1346.29it/s]\n",
      "607it [00:00, 1260.95it/s]\n",
      "100%|████████████████████████████████████| 5375/5375 [00:00<00:00, 74385.74it/s]\n",
      "=================================\n",
      "BLEU test: 0.029993427458531466\n"
     ]
    }
   ],
   "source": [
    "!python training.py --name_file=Educativo/all/educativo_10000  --dir_files=ParallelCorpora --dir_results=results_educativo_{folder}/bpe_10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "4768it [00:03, 1251.04it/s]\n",
      "607it [00:00, 1350.28it/s]\n",
      "607it [00:00, 1264.09it/s]\n",
      "100%|████████████████████████████████████| 5375/5375 [00:00<00:00, 75290.25it/s]\n",
      "=================================\n",
      "BLEU test: 0.005327791414768956\n"
     ]
    }
   ],
   "source": [
    "!python training.py --name_file=Educativo/all/educativo_15000  --dir_files=ParallelCorpora --dir_results=results_educativo/bpe_15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "4768it [00:03, 1266.83it/s]\n",
      "607it [00:00, 1370.95it/s]\n",
      "607it [00:00, 1280.84it/s]\n",
      "100%|████████████████████████████████████| 5375/5375 [00:00<00:00, 76347.88it/s]\n",
      "=================================\n",
      "BLEU test: 0.03230557350632636\n"
     ]
    }
   ],
   "source": [
    "!python training.py --name_file=Educativo/all/educativo_20000  --dir_files=ParallelCorpora --dir_results=results_educativo/bpe_20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Religioso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "10021it [00:15, 653.98it/s]\n",
      "1263it [00:01, 680.90it/s]\n",
      "1263it [00:02, 611.34it/s]\n",
      "100%|██████████████████████████████████| 11284/11284 [00:00<00:00, 42879.48it/s]\n",
      "=================================\n",
      "BLEU test: 0.006334807962230557\n"
     ]
    }
   ],
   "source": [
    "!python training.py --name_file=Religioso/all/religioso_5000  --dir_files=ParallelCorpora --dir_results=results_religioso/bpe_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "10021it [00:14, 713.46it/s]\n",
      "1263it [00:01, 744.85it/s]\n",
      "1263it [00:01, 725.51it/s]\n",
      "100%|██████████████████████████████████| 11284/11284 [00:00<00:00, 44782.97it/s]\n",
      "=================================\n",
      "BLEU test: 0.008752968583175471\n"
     ]
    }
   ],
   "source": [
    "!python training.py --name_file=Religioso/all/religioso_10000  --dir_files=ParallelCorpora --dir_results=results_religioso/bpe_10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "10021it [00:13, 756.17it/s]\n",
      "1263it [00:01, 728.26it/s]\n",
      "1263it [00:01, 768.26it/s]\n",
      "100%|██████████████████████████████████| 11284/11284 [00:00<00:00, 47345.48it/s]\n",
      "=================================\n",
      "BLEU test: 0.007384567420679714\n"
     ]
    }
   ],
   "source": [
    "!python training.py --name_file=Religioso/all/religioso_15000  --dir_files=ParallelCorpora --dir_results=results_religioso/bpe_15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "10021it [00:12, 781.68it/s]\n",
      "1263it [00:01, 750.26it/s]\n",
      "1263it [00:01, 789.79it/s]\n",
      "100%|██████████████████████████████████| 11284/11284 [00:00<00:00, 47823.16it/s]\n",
      "=================================\n",
      "BLEU test: 0.0023481668446239013\n"
     ]
    }
   ],
   "source": [
    "!python training.py --name_file=Religioso/all/religioso_20000  --dir_files=ParallelCorpora --dir_results=results_religioso/bpe_20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flashcards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "4876it [00:02, 2234.44it/s]\n",
      "774it [00:00, 2413.06it/s]\n",
      "2090it [00:00, 2170.95it/s]\n",
      "100%|███████████████████████████████████| 5650/5650 [00:00<00:00, 127262.57it/s]\n",
      "=================================\n",
      "BLEU test: 0.014600467404777016\n"
     ]
    }
   ],
   "source": [
    "!python training.py --name_file=Flashcards/all/flashcards_5000  --dir_files=ParallelCorpora --dir_results=results_flashcards/bpe_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "4876it [00:02, 2368.69it/s]\n",
      "774it [00:00, 2690.60it/s]\n",
      "2090it [00:00, 2298.32it/s]\n",
      "100%|███████████████████████████████████| 5650/5650 [00:00<00:00, 128957.19it/s]\n",
      "=================================\n",
      "BLEU test: 0.012546480745815363\n"
     ]
    }
   ],
   "source": [
    "!python training.py --name_file=Flashcards/all/flashcards_10000  --dir_files=ParallelCorpora --dir_results=results_flashcards/bpe_10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "4876it [00:02, 2411.90it/s]\n",
      "774it [00:00, 2733.44it/s]\n",
      "2090it [00:00, 2326.05it/s]\n",
      "100%|███████████████████████████████████| 5650/5650 [00:00<00:00, 133234.11it/s]\n",
      "=================================\n",
      "BLEU test: 1.0945440478630852e-233\n"
     ]
    }
   ],
   "source": [
    "!python training.py --name_file=Flashcards/all/flashcards_15000  --dir_files=ParallelCorpora --dir_results=results_flashcards/bpe_15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "4876it [00:01, 2633.18it/s]\n",
      "774it [00:00, 2716.21it/s]\n",
      "2090it [00:00, 2313.81it/s]\n",
      "100%|███████████████████████████████████| 5650/5650 [00:00<00:00, 127421.32it/s]\n",
      "=================================\n",
      "BLEU test: 0.012360495449667473\n"
     ]
    }
   ],
   "source": [
    "!python training.py --name_file=Flashcards/all/flashcards_20000  --dir_files=ParallelCorpora --dir_results=results_flashcards/bpe_20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     j       | 5000   | 10000  | 15000   | 20000  |\n",
    "|------------|--------|--------|---------|--------|\n",
    "| Educativo  | 0.0353 | 0.0299 | 0.0053  | 0.0323 |\n",
    "| Religioso  | 0.0063 | 0.0087 | 0.0073  | 0.0023 |\n",
    "| Flashcards | 0.0146 | 0.0125 | 0.00001 | 0.0123 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
